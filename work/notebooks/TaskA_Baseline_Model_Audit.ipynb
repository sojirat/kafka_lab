{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task A: Baseline Model & Offline Audit\n",
    "\n",
    "**Objectives:**\n",
    "1. Perform data audit (rows, columns, duplicates, class imbalance)\n",
    "2. Train baseline fraud detection model\n",
    "3. Report PR-AUC and recall at chosen threshold\n",
    "4. Save model for streaming inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Path: /home/jovyan/work/data/creditcard.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setup paths\n",
    "WORK = Path(\"/home/jovyan/work\")\n",
    "DATA = WORK / \"data\"\n",
    "MODELS = WORK / \"models\"\n",
    "AUDIT = WORK / \"audit_results\"\n",
    "\n",
    "for p in [DATA, MODELS, AUDIT]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "csv_path = str(DATA / \"creditcard.csv\")\n",
    "print(f\"CSV Path: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA AUDIT REPORT\n",
      "================================================================================\n",
      "\n",
      "1. DATASET DIMENSIONS\n",
      "   - Number of rows: 284,807\n",
      "   - Number of columns: 31\n",
      "\n",
      "   Column names: ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Class']\n"
     ]
    }
   ],
   "source": [
    "# Load data with pandas for quick audit\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA AUDIT REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Basic dimensions\n",
    "n_rows, n_cols = df.shape\n",
    "print(f\"\\n1. DATASET DIMENSIONS\")\n",
    "print(f\"   - Number of rows: {n_rows:,}\")\n",
    "print(f\"   - Number of columns: {n_cols}\")\n",
    "print(f\"\\n   Column names: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. DUPLICATE ANALYSIS\n",
      "   - Duplicates (before drop): 1,081 (0.3796%)\n",
      "   - Duplicates removed: 1,081\n",
      "   - Rows after drop: 283,726\n"
     ]
    }
   ],
   "source": [
    "# 2. Duplicates analysis\n",
    "n_duplicates_before = df.duplicated().sum()\n",
    "duplicate_pct_before = (n_duplicates_before / n_rows) * 100\n",
    "\n",
    "print(f\"\\n2. DUPLICATE ANALYSIS\")\n",
    "print(f\"   - Duplicates (before drop): {n_duplicates_before:,} ({duplicate_pct_before:.4f}%)\")\n",
    "\n",
    "# Drop duplicates\n",
    "df_clean = df.drop_duplicates()\n",
    "n_rows_after = len(df_clean)\n",
    "n_duplicates_removed = n_rows - n_rows_after\n",
    "\n",
    "print(f\"   - Duplicates removed: {n_duplicates_removed:,}\")\n",
    "print(f\"   - Rows after drop: {n_rows_after:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. CLASS IMBALANCE\n",
      "   - Class 0 (Normal): 283,253 (99.8333%)\n",
      "   - Class 1 (Fraud):  473 (0.1667%)\n",
      "   - Imbalance ratio (0:1): 598.84:1\n",
      "   - This is a HIGHLY IMBALANCED dataset\n"
     ]
    }
   ],
   "source": [
    "# 3. Class imbalance\n",
    "class_counts = df_clean['Class'].value_counts().sort_index()\n",
    "class_0 = class_counts[0]\n",
    "class_1 = class_counts[1]\n",
    "total = class_0 + class_1\n",
    "\n",
    "pct_0 = (class_0 / total) * 100\n",
    "pct_1 = (class_1 / total) * 100\n",
    "imbalance_ratio = class_0 / class_1\n",
    "\n",
    "print(f\"\\n3. CLASS IMBALANCE\")\n",
    "print(f\"   - Class 0 (Normal): {class_0:,} ({pct_0:.4f}%)\")\n",
    "print(f\"   - Class 1 (Fraud):  {class_1:,} ({pct_1:.4f}%)\")\n",
    "print(f\"   - Imbalance ratio (0:1): {imbalance_ratio:.2f}:1\")\n",
    "print(f\"   - This is a HIGHLY IMBALANCED dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. MISSING VALUES\n",
      "   - Total missing cells: 0\n",
      "   - No missing values detected\n"
     ]
    }
   ],
   "source": [
    "# 4. Missing values\n",
    "missing_counts = df_clean.isnull().sum()\n",
    "total_missing = missing_counts.sum()\n",
    "\n",
    "print(f\"\\n4. MISSING VALUES\")\n",
    "print(f\"   - Total missing cells: {total_missing}\")\n",
    "if total_missing > 0:\n",
    "    print(f\"\\n   Columns with missing values:\")\n",
    "    for col, count in missing_counts[missing_counts > 0].items():\n",
    "        print(f\"      {col}: {count}\")\n",
    "else:\n",
    "    print(f\"   - No missing values detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. STATISTICAL SUMMARY\n",
      "\n",
      "Amount column statistics:\n",
      "count    283726.000000\n",
      "mean         88.472687\n",
      "std         250.399437\n",
      "min           0.000000\n",
      "25%           5.600000\n",
      "50%          22.000000\n",
      "75%          77.510000\n",
      "max       25691.160000\n",
      "Name: Amount, dtype: float64\n",
      "\n",
      "Time column statistics:\n",
      "count    283726.000000\n",
      "mean      94811.077600\n",
      "std       47481.047891\n",
      "min           0.000000\n",
      "25%       54204.750000\n",
      "50%       84692.500000\n",
      "75%      139298.000000\n",
      "max      172792.000000\n",
      "Name: Time, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 5. Statistical summary for key fields\n",
    "print(f\"\\n5. STATISTICAL SUMMARY\")\n",
    "print(f\"\\nAmount column statistics:\")\n",
    "print(df_clean['Amount'].describe())\n",
    "\n",
    "print(f\"\\nTime column statistics:\")\n",
    "print(df_clean['Time'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "AUDIT SUMMARY TABLE\n",
      "================================================================================\n",
      "               Metric    Value\n",
      "Total Rows (Original)  284,807\n",
      "        Total Columns       31\n",
      "     Duplicates Found    1,081\n",
      "         Duplicates %  0.3796%\n",
      "     Rows After Dedup  283,726\n",
      "       Missing Values        0\n",
      "        Class 0 Count  283,253\n",
      "            Class 0 % 99.8333%\n",
      "        Class 1 Count      473\n",
      "            Class 1 %  0.1667%\n",
      "Imbalance Ratio (0:1) 598.84:1\n",
      "\n",
      "Audit summary saved to: /home/jovyan/work/audit_results/data_audit_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# 6. Create audit summary table\n",
    "audit_summary = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Total Rows (Original)',\n",
    "        'Total Columns',\n",
    "        'Duplicates Found',\n",
    "        'Duplicates %',\n",
    "        'Rows After Dedup',\n",
    "        'Missing Values',\n",
    "        'Class 0 Count',\n",
    "        'Class 0 %',\n",
    "        'Class 1 Count',\n",
    "        'Class 1 %',\n",
    "        'Imbalance Ratio (0:1)'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{n_rows:,}\",\n",
    "        f\"{n_cols}\",\n",
    "        f\"{n_duplicates_before:,}\",\n",
    "        f\"{duplicate_pct_before:.4f}%\",\n",
    "        f\"{n_rows_after:,}\",\n",
    "        f\"{total_missing}\",\n",
    "        f\"{class_0:,}\",\n",
    "        f\"{pct_0:.4f}%\",\n",
    "        f\"{class_1:,}\",\n",
    "        f\"{pct_1:.4f}%\",\n",
    "        f\"{imbalance_ratio:.2f}:1\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AUDIT SUMMARY TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(audit_summary.to_string(index=False))\n",
    "\n",
    "# Save audit summary\n",
    "audit_summary.to_csv(AUDIT / \"data_audit_summary.csv\", index=False)\n",
    "print(f\"\\nAudit summary saved to: {AUDIT / 'data_audit_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Model Training\n",
    "\n",
    "We will train two models:\n",
    "1. **Logistic Regression** (fast, interpretable)\n",
    "2. **Random Forest** (tree-based, handles non-linearity)\n",
    "\n",
    "Both will use class weights to handle imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"TaskA-Baseline-Model\")\n",
    "         .master(\"local[*]\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns (29): ['V1', 'V2', 'V3', 'V4', 'V5'] ... ['V28', 'Amount']\n",
      "Label column: Class\n"
     ]
    }
   ],
   "source": [
    "# Load clean data to Spark\n",
    "sdf = spark.createDataFrame(df_clean)\n",
    "\n",
    "# Define features and label\n",
    "feature_cols = [f\"V{i}\" for i in range(1, 29)] + [\"Amount\"]\n",
    "label_col = \"Class\"\n",
    "\n",
    "print(f\"Feature columns ({len(feature_cols)}): {feature_cols[:5]} ... {feature_cols[-2:]}\")\n",
    "print(f\"Label column: {label_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 227,157\n",
      "Test set size: 56,569\n"
     ]
    }
   ],
   "source": [
    "# Split data: 80% train, 20% test\n",
    "train_sdf, test_sdf = sdf.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training set size: {train_sdf.count():,}\")\n",
    "print(f\"Test set size: {test_sdf.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set class distribution:\n",
      "   Class 0: 226,792\n",
      "   Class 1: 365\n",
      "   Weight for class 1 (fraud): 621.35\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights for training set\n",
    "train_class_counts = train_sdf.groupBy(label_col).count().collect()\n",
    "counts_dict = {row[label_col]: row[\"count\"] for row in train_class_counts}\n",
    "\n",
    "n_class_0 = counts_dict.get(0, 1)\n",
    "n_class_1 = counts_dict.get(1, 1)\n",
    "weight_class_1 = n_class_0 / n_class_1\n",
    "weight_class_0 = 1.0\n",
    "\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(f\"   Class 0: {n_class_0:,}\")\n",
    "print(f\"   Class 1: {n_class_1:,}\")\n",
    "print(f\"   Weight for class 1 (fraud): {weight_class_1:.2f}\")\n",
    "\n",
    "# Add weight column\n",
    "train_sdf = train_sdf.withColumn(\n",
    "    \"weight\",\n",
    "    F.when(F.col(label_col) == 1, F.lit(float(weight_class_1))).otherwise(F.lit(float(weight_class_0)))\n",
    ")\n",
    "\n",
    "test_sdf = test_sdf.withColumn(\n",
    "    \"weight\",\n",
    "    F.when(F.col(label_col) == 1, F.lit(float(weight_class_1))).otherwise(F.lit(float(weight_class_0)))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression model...\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Build Logistic Regression pipeline\n",
    "assembler_lr = VectorAssembler(inputCols=feature_cols, outputCol=\"features_raw\")\n",
    "scaler_lr = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=label_col,\n",
    "    weightCol=\"weight\",\n",
    "    maxIter=100,\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=0.0  # L2 regularization\n",
    ")\n",
    "\n",
    "pipeline_lr = Pipeline(stages=[assembler_lr, scaler_lr, lr])\n",
    "\n",
    "print(\"Training Logistic Regression model...\")\n",
    "model_lr = pipeline_lr.fit(train_sdf)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Performance:\n",
      "   PR-AUC:  0.632238\n",
      "   ROC-AUC: 0.965672\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Logistic Regression\n",
    "pred_lr = model_lr.transform(test_sdf)\n",
    "\n",
    "evaluator_pr = BinaryClassificationEvaluator(\n",
    "    labelCol=label_col,\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderPR\"\n",
    ")\n",
    "\n",
    "evaluator_roc = BinaryClassificationEvaluator(\n",
    "    labelCol=label_col,\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "pr_auc_lr = evaluator_pr.evaluate(pred_lr)\n",
    "roc_auc_lr = evaluator_roc.evaluate(pred_lr)\n",
    "\n",
    "print(f\"\\nLogistic Regression Performance:\")\n",
    "print(f\"   PR-AUC:  {pr_auc_lr:.6f}\")\n",
    "print(f\"   ROC-AUC: {roc_auc_lr:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest model...\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Build Random Forest pipeline\n",
    "assembler_rf = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=label_col,\n",
    "    weightCol=\"weight\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline_rf = Pipeline(stages=[assembler_rf, rf])\n",
    "\n",
    "print(\"Training Random Forest model...\")\n",
    "model_rf = pipeline_rf.fit(train_sdf)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Performance:\n",
      "   PR-AUC:  0.784108\n",
      "   ROC-AUC: 0.957809\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Random Forest\n",
    "pred_rf = model_rf.transform(test_sdf)\n",
    "\n",
    "pr_auc_rf = evaluator_pr.evaluate(pred_rf)\n",
    "roc_auc_rf = evaluator_roc.evaluate(pred_rf)\n",
    "\n",
    "print(f\"\\nRandom Forest Performance:\")\n",
    "print(f\"   PR-AUC:  {pr_auc_rf:.6f}\")\n",
    "print(f\"   ROC-AUC: {roc_auc_rf:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Threshold Selection & Recall Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fraud probability for both models\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def extract_prob(v):\n",
    "    try:\n",
    "        return float(v[1])\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "extract_prob_udf = F.udf(extract_prob, DoubleType())\n",
    "\n",
    "pred_lr = pred_lr.withColumn(\"fraud_prob\", extract_prob_udf(F.col(\"probability\")))\n",
    "pred_rf = pred_rf.withColumn(\"fraud_prob\", extract_prob_udf(F.col(\"probability\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "THRESHOLD ANALYSIS - LOGISTIC REGRESSION\n",
      "================================================================================\n",
      "Threshold: 0.30 | Precision: 0.0338 | Recall: 0.8981 | F1: 0.0651 | TP: 97 | FP: 2773 | FN: 11\n",
      "Threshold: 0.50 | Precision: 0.1141 | Recall: 0.8611 | F1: 0.2015 | TP: 93 | FP: 722 | FN: 15\n",
      "Threshold: 0.70 | Precision: 0.3061 | Recall: 0.8333 | F1: 0.4478 | TP: 90 | FP: 204 | FN: 18\n",
      "Threshold: 0.80 | Precision: 0.5176 | Recall: 0.8148 | F1: 0.6331 | TP: 88 | FP: 82 | FN: 20\n",
      "Threshold: 0.90 | Precision: 0.6343 | Recall: 0.7870 | F1: 0.7025 | TP: 85 | FP: 49 | FN: 23\n",
      "Threshold: 0.95 | Precision: 0.6875 | Recall: 0.7130 | F1: 0.7000 | TP: 77 | FP: 35 | FN: 31\n",
      "Threshold: 0.99 | Precision: 0.7558 | Recall: 0.6019 | F1: 0.6701 | TP: 65 | FP: 21 | FN: 43\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate precision, recall at different thresholds\n",
    "def calc_metrics_at_threshold(pred_df, threshold):\n",
    "    pred_with_alert = pred_df.withColumn(\n",
    "        \"predicted\",\n",
    "        F.when(F.col(\"fraud_prob\") >= threshold, 1).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # Calculate TP, FP, TN, FN\n",
    "    metrics = pred_with_alert.groupBy().agg(\n",
    "        F.sum(F.when((F.col(\"predicted\") == 1) & (F.col(label_col) == 1), 1).otherwise(0)).alias(\"tp\"),\n",
    "        F.sum(F.when((F.col(\"predicted\") == 1) & (F.col(label_col) == 0), 1).otherwise(0)).alias(\"fp\"),\n",
    "        F.sum(F.when((F.col(\"predicted\") == 0) & (F.col(label_col) == 0), 1).otherwise(0)).alias(\"tn\"),\n",
    "        F.sum(F.when((F.col(\"predicted\") == 0) & (F.col(label_col) == 1), 1).otherwise(0)).alias(\"fn\"),\n",
    "        F.sum(F.col(label_col)).alias(\"total_fraud\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    tp = metrics[\"tp\"] or 0\n",
    "    fp = metrics[\"fp\"] or 0\n",
    "    tn = metrics[\"tn\"] or 0\n",
    "    fn = metrics[\"fn\"] or 0\n",
    "    total_fraud = metrics[\"total_fraud\"] or 1\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / total_fraud if total_fraud > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"threshold\": threshold,\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"tn\": tn,\n",
    "        \"fn\": fn,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# Test multiple thresholds\n",
    "thresholds = [0.3, 0.5, 0.7, 0.8, 0.9, 0.95, 0.99]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"THRESHOLD ANALYSIS - LOGISTIC REGRESSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_lr = []\n",
    "for thresh in thresholds:\n",
    "    metrics = calc_metrics_at_threshold(pred_lr, thresh)\n",
    "    results_lr.append(metrics)\n",
    "    print(f\"Threshold: {thresh:.2f} | Precision: {metrics['precision']:.4f} | Recall: {metrics['recall']:.4f} | F1: {metrics['f1']:.4f} | TP: {metrics['tp']} | FP: {metrics['fp']} | FN: {metrics['fn']}\")\n",
    "\n",
    "df_results_lr = pd.DataFrame(results_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "THRESHOLD ANALYSIS - RANDOM FOREST\n",
      "================================================================================\n",
      "Threshold: 0.30 | Precision: 0.7236 | Recall: 0.8241 | F1: 0.7706 | TP: 89 | FP: 34 | FN: 19\n",
      "Threshold: 0.50 | Precision: 0.8300 | Recall: 0.7685 | F1: 0.7981 | TP: 83 | FP: 17 | FN: 25\n",
      "Threshold: 0.70 | Precision: 0.8621 | Recall: 0.6944 | F1: 0.7692 | TP: 75 | FP: 12 | FN: 33\n",
      "Threshold: 0.80 | Precision: 0.8816 | Recall: 0.6204 | F1: 0.7283 | TP: 67 | FP: 9 | FN: 41\n",
      "Threshold: 0.90 | Precision: 0.9265 | Recall: 0.5833 | F1: 0.7159 | TP: 63 | FP: 5 | FN: 45\n",
      "Threshold: 0.95 | Precision: 0.9492 | Recall: 0.5185 | F1: 0.6707 | TP: 56 | FP: 3 | FN: 52\n",
      "Threshold: 0.99 | Precision: 1.0000 | Recall: 0.2315 | F1: 0.3759 | TP: 25 | FP: 0 | FN: 83\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"THRESHOLD ANALYSIS - RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_rf = []\n",
    "for thresh in thresholds:\n",
    "    metrics = calc_metrics_at_threshold(pred_rf, thresh)\n",
    "    results_rf.append(metrics)\n",
    "    print(f\"Threshold: {thresh:.2f} | Precision: {metrics['precision']:.4f} | Recall: {metrics['recall']:.4f} | F1: {metrics['f1']:.4f} | TP: {metrics['tp']} | FP: {metrics['fp']} | FN: {metrics['fn']}\")\n",
    "\n",
    "df_results_rf = pd.DataFrame(results_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Select Best Model & Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BEST THRESHOLD SELECTION (Based on F1 Score)\n",
      "================================================================================\n",
      "\n",
      "Logistic Regression:\n",
      "   Best Threshold: 0.90\n",
      "   Precision: 0.6343\n",
      "   Recall: 0.7870\n",
      "   F1 Score: 0.7025\n",
      "   PR-AUC: 0.632238\n",
      "\n",
      "Random Forest:\n",
      "   Best Threshold: 0.50\n",
      "   Precision: 0.8300\n",
      "   Recall: 0.7685\n",
      "   F1 Score: 0.7981\n",
      "   PR-AUC: 0.784108\n"
     ]
    }
   ],
   "source": [
    "# Choose threshold based on F1 score\n",
    "best_lr = df_results_lr.loc[df_results_lr['f1'].idxmax()]\n",
    "best_rf = df_results_rf.loc[df_results_rf['f1'].idxmax()]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST THRESHOLD SELECTION (Based on F1 Score)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nLogistic Regression:\")\n",
    "print(f\"   Best Threshold: {best_lr['threshold']:.2f}\")\n",
    "print(f\"   Precision: {best_lr['precision']:.4f}\")\n",
    "print(f\"   Recall: {best_lr['recall']:.4f}\")\n",
    "print(f\"   F1 Score: {best_lr['f1']:.4f}\")\n",
    "print(f\"   PR-AUC: {pr_auc_lr:.6f}\")\n",
    "\n",
    "print(f\"\\nRandom Forest:\")\n",
    "print(f\"   Best Threshold: {best_rf['threshold']:.2f}\")\n",
    "print(f\"   Precision: {best_rf['precision']:.4f}\")\n",
    "print(f\"   Recall: {best_rf['recall']:.4f}\")\n",
    "print(f\"   F1 Score: {best_rf['f1']:.4f}\")\n",
    "print(f\"   PR-AUC: {pr_auc_rf:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL MODEL SELECTION\n",
      "================================================================================\n",
      "\n",
      "Selected Model: Random Forest\n",
      "PR-AUC: 0.784108\n",
      "\n",
      "Recommended Operating Point:\n",
      "   Threshold: 0.50\n",
      "   Precision: 0.8300\n",
      "   Recall: 0.7685\n",
      "   F1 Score: 0.7981\n",
      "\n",
      "Rationale for threshold selection:\n",
      "   - Threshold 0.50 maximizes F1 score\n",
      "   - Balances precision (83.00%) and recall (76.85%)\n",
      "   - In production, adjust based on business cost of false positives vs false negatives\n"
     ]
    }
   ],
   "source": [
    "# Select final model (based on PR-AUC)\n",
    "if pr_auc_lr >= pr_auc_rf:\n",
    "    final_model = model_lr\n",
    "    final_model_name = \"Logistic Regression\"\n",
    "    final_pr_auc = pr_auc_lr\n",
    "    final_threshold = best_lr['threshold']\n",
    "    final_precision = best_lr['precision']\n",
    "    final_recall = best_lr['recall']\n",
    "    final_f1 = best_lr['f1']\n",
    "else:\n",
    "    final_model = model_rf\n",
    "    final_model_name = \"Random Forest\"\n",
    "    final_pr_auc = pr_auc_rf\n",
    "    final_threshold = best_rf['threshold']\n",
    "    final_precision = best_rf['precision']\n",
    "    final_recall = best_rf['recall']\n",
    "    final_f1 = best_rf['f1']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL MODEL SELECTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSelected Model: {final_model_name}\")\n",
    "print(f\"PR-AUC: {final_pr_auc:.6f}\")\n",
    "print(f\"\\nRecommended Operating Point:\")\n",
    "print(f\"   Threshold: {final_threshold:.2f}\")\n",
    "print(f\"   Precision: {final_precision:.4f}\")\n",
    "print(f\"   Recall: {final_recall:.4f}\")\n",
    "print(f\"   F1 Score: {final_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nRationale for threshold selection:\")\n",
    "print(f\"   - Threshold {final_threshold:.2f} maximizes F1 score\")\n",
    "print(f\"   - Balances precision ({final_precision:.2%}) and recall ({final_recall:.2%})\")\n",
    "print(f\"   - In production, adjust based on business cost of false positives vs false negatives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Models & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model saved to: /home/jovyan/work/models/fraud_lr_model\n",
      "Random Forest model saved to: /home/jovyan/work/models/fraud_rf_model\n",
      "\n",
      "Threshold analysis saved to: /home/jovyan/work/audit_results\n",
      "Model selection summary saved to: /home/jovyan/work/audit_results/model_selection_summary.json\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Save Logistic Regression model\n",
    "model_lr_path = str(MODELS / \"fraud_lr_model\")\n",
    "if os.path.exists(model_lr_path):\n",
    "    shutil.rmtree(model_lr_path)\n",
    "model_lr.write().overwrite().save(model_lr_path)\n",
    "print(f\"Logistic Regression model saved to: {model_lr_path}\")\n",
    "\n",
    "# Save Random Forest model\n",
    "model_rf_path = str(MODELS / \"fraud_rf_model\")\n",
    "if os.path.exists(model_rf_path):\n",
    "    shutil.rmtree(model_rf_path)\n",
    "model_rf.write().overwrite().save(model_rf_path)\n",
    "print(f\"Random Forest model saved to: {model_rf_path}\")\n",
    "\n",
    "# Save threshold analysis results\n",
    "df_results_lr.to_csv(AUDIT / \"threshold_analysis_lr.csv\", index=False)\n",
    "df_results_rf.to_csv(AUDIT / \"threshold_analysis_rf.csv\", index=False)\n",
    "print(f\"\\nThreshold analysis saved to: {AUDIT}\")\n",
    "\n",
    "# Save final model selection summary\n",
    "final_summary = {\n",
    "    \"selected_model\": final_model_name,\n",
    "    \"pr_auc\": float(final_pr_auc),\n",
    "    \"recommended_threshold\": float(final_threshold),\n",
    "    \"precision_at_threshold\": float(final_precision),\n",
    "    \"recall_at_threshold\": float(final_recall),\n",
    "    \"f1_at_threshold\": float(final_f1),\n",
    "    \"model_comparison\": {\n",
    "        \"logistic_regression_pr_auc\": float(pr_auc_lr),\n",
    "        \"random_forest_pr_auc\": float(pr_auc_rf)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(AUDIT / \"model_selection_summary.json\", \"w\") as f:\n",
    "    json.dump(final_summary, f, indent=2)\n",
    "\n",
    "print(f\"Model selection summary saved to: {AUDIT / 'model_selection_summary.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Task A Deliverables Summary\n",
    "\n",
    "### Deliverable A.1: Audit Summary Table\n",
    "See audit summary table above and saved CSV file.\n",
    "\n",
    "### Deliverable A.2: Model Performance\n",
    "- **Selected Model:** See final model selection above\n",
    "- **PR-AUC:** Reported above\n",
    "- **Recommended Threshold:** Selected based on F1 score optimization\n",
    "- **Rationale:** Threshold balances precision and recall. In production, adjust based on:\n",
    "  - Cost of false positives (operational overhead of reviewing alerts)\n",
    "  - Cost of false negatives (financial loss from missed fraud)\n",
    "  - Alert volume capacity of fraud investigation team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK A COMPLETE\n",
      "================================================================================\n",
      "\n",
      "All deliverables saved to:\n",
      "   - Audit summary: /home/jovyan/work/audit_results/data_audit_summary.csv\n",
      "   - Threshold analysis: /home/jovyan/work/audit_results\n",
      "   - Model selection: /home/jovyan/work/audit_results/model_selection_summary.json\n",
      "   - Models: /home/jovyan/work/models\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK A COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAll deliverables saved to:\")\n",
    "print(f\"   - Audit summary: {AUDIT / 'data_audit_summary.csv'}\")\n",
    "print(f\"   - Threshold analysis: {AUDIT}\")\n",
    "print(f\"   - Model selection: {AUDIT / 'model_selection_summary.json'}\")\n",
    "print(f\"   - Models: {MODELS}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
