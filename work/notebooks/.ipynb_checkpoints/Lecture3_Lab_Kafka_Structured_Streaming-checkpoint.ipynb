{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00474840",
   "metadata": {},
   "source": [
    "# Lecture 3 Lab — Kafka + Spark Structured Streaming (Real-time Fraud Detection)\n",
    "\n",
    "**Data:** `creditcard.csv`  \n",
    "**Kafka topics:** `transactions` (input) → `fraud_alerts` (output)  \n",
    "\n",
    "Lab outcomes:\n",
    "- Train a baseline fraud model offline (Spark ML)\n",
    "- Stream transactions from Kafka and score in real time\n",
    "- Windowed KPI analytics with event-time + watermark\n",
    "- Fault tolerance with checkpointing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d906967a",
   "metadata": {},
   "source": [
    "## 0) Paths & versions\n",
    "This notebook is designed for the Docker Compose package:\n",
    "- Jupyter image: `jupyter/pyspark-notebook:spark-3.5.1`\n",
    "- Kafka image: `bitnami/kafka:3.7`\n",
    "\n",
    "Folder mapping:\n",
    "- Host `./work` → container `/home/jovyan/work`\n",
    "- Host `./data/creditcard.csv` → available in the package (already copied)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2252f5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/data/creditcard.csv'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "WORK = Path(\"/home/jovyan/work\")\n",
    "DATA = WORK / \"data\"\n",
    "MODELS = WORK / \"models\"\n",
    "OUT = WORK / \"output_alerts\"\n",
    "CKPT = WORK / \"checkpoints\"\n",
    "\n",
    "for p in [DATA, MODELS, OUT, CKPT]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "csv_path = str(DATA / \"creditcard.csv\")\n",
    "csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e166e852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, PosixPath('/home/jovyan/work/data/creditcard.csv'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil, pathlib\n",
    "\n",
    "# If the dataset is missing under /home/jovyan/work/data, try to copy from /data (if you mounted it)\n",
    "fallback = pathlib.Path(\"/data/creditcard.csv\")\n",
    "target = pathlib.Path(csv_path)\n",
    "if (not target.exists()) and fallback.exists():\n",
    "    shutil.copy2(fallback, target)\n",
    "\n",
    "target.exists(), target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c876a",
   "metadata": {},
   "source": [
    "## 1) Start Spark with Kafka connector\n",
    "Spark needs the Kafka connector package. The coordinates must match your Spark version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b335e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.5.0', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "\n",
    "spark_version = pyspark.__version__\n",
    "# For Spark 3.5.1 notebook image:\n",
    "kafka_pkg = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{spark_version}\"\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Lecture3-Kafka-Fraud\")\n",
    "         .master(\"local[*]\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "         .config(\"spark.jars.packages\", kafka_pkg)\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "spark_version, kafka_pkg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0fd4d7",
   "metadata": {},
   "source": [
    "## 2) Offline data audit + prepare event_time\n",
    "Audit: duplicates, missing values, and class imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "681c69b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (284807, 31)\n",
      "Duplicates: 1081\n",
      "Missing cells: 0\n",
      "Class distribution:\n",
      "Class\n",
      "0    284315\n",
      "1       492\n",
      "Name: count, dtype: int64\n",
      "Class\n",
      "0    99.8273\n",
      "1     0.1727\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>event_time</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-12-16 12:46:03</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-12-16 12:46:03</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2025-12-16 12:46:04</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2025-12-16 12:46:04</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2025-12-16 12:46:05</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time          event_time  Amount  Class\n",
       "0   0.0 2025-12-16 12:46:03  149.62      0\n",
       "1   0.0 2025-12-16 12:46:03    2.69      0\n",
       "2   1.0 2025-12-16 12:46:04  378.66      0\n",
       "3   1.0 2025-12-16 12:46:04  123.50      0\n",
       "4   2.0 2025-12-16 12:46:05   69.99      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "pdf = pd.read_csv(csv_path)\n",
    "print(\"Shape:\", pdf.shape)\n",
    "print(\"Duplicates:\", pdf.duplicated().sum())\n",
    "print(\"Missing cells:\", int(pdf.isna().sum().sum()))\n",
    "print(\"Class distribution:\")\n",
    "print(pdf[\"Class\"].value_counts())\n",
    "print((pdf[\"Class\"].value_counts(normalize=True)*100).round(4))\n",
    "\n",
    "pdf = pdf.drop_duplicates()\n",
    "\n",
    "# Create event_time (timestamp) for streaming windows\n",
    "start_ts = datetime.datetime.now().replace(microsecond=0)\n",
    "pdf[\"event_time\"] = pd.to_datetime(start_ts) + pd.to_timedelta(pdf[\"Time\"], unit=\"s\")\n",
    "\n",
    "pdf[[\"Time\",\"event_time\",\"Amount\",\"Class\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623f1bb1",
   "metadata": {},
   "source": [
    "## 3) Train and save a baseline model (Spark ML)\n",
    "Pipeline: assembler → scaler → logistic regression. Use `weightCol` for rare fraud.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ea7548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: {0: 226792, 1: 365} fraud_weight: 621.3479452054795\n",
      "PR-AUC: 0.6973528063247361\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "sdf = spark.createDataFrame(pdf)\n",
    "\n",
    "feature_cols = [c for c in sdf.columns if c.startswith(\"V\")] + [\"Amount\"]\n",
    "label_col = \"Class\"\n",
    "\n",
    "train, test = sdf.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "counts = train.groupBy(label_col).count().collect()\n",
    "counts = {row[label_col]: row[\"count\"] for row in counts}\n",
    "w0 = 1.0\n",
    "w1 = counts.get(0,1) / max(counts.get(1,1), 1)\n",
    "print(\"Class counts:\", counts, \"fraud_weight:\", w1)\n",
    "\n",
    "train = train.withColumn(\"weight\", F.when(F.col(label_col)==1, F.lit(float(w1))).otherwise(F.lit(float(w0))))\n",
    "test  = test.withColumn(\"weight\",  F.when(F.col(label_col)==1, F.lit(float(w1))).otherwise(F.lit(float(w0))))\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_raw\")\n",
    "scaler    = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "lr        = LogisticRegression(featuresCol=\"features\", labelCol=label_col, weightCol=\"weight\", maxIter=50)\n",
    "\n",
    "pipe = Pipeline(stages=[assembler, scaler, lr])\n",
    "model = pipe.fit(train)\n",
    "\n",
    "pred = model.transform(test)\n",
    "pr_auc = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderPR\").evaluate(pred)\n",
    "print(\"PR-AUC:\", pr_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "732c2b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/models/fraud_lr_model'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil, os\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "model_path = str(MODELS / \"fraud_lr_model\")\n",
    "if os.path.exists(model_path):\n",
    "    shutil.rmtree(model_path)\n",
    "\n",
    "model.write().overwrite().save(model_path)\n",
    "model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d040cca0",
   "metadata": {},
   "source": [
    "## 4) Read transactions from Kafka\n",
    "Producer sends JSON messages with fields:\n",
    "`event_time, Time, V1..V28, Amount, Class`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92cfdeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "schema_fields = [\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"Time\", DoubleType(), True),\n",
    "]\n",
    "for i in range(1, 29):\n",
    "    schema_fields.append(StructField(f\"V{i}\", DoubleType(), True))\n",
    "schema_fields += [\n",
    "    StructField(\"Amount\", DoubleType(), True),\n",
    "    StructField(\"Class\", IntegerType(), True),\n",
    "]\n",
    "txn_schema = StructType(schema_fields)\n",
    "\n",
    "BOOTSTRAP = \"kafka:9092\"\n",
    "TOPIC_IN  = \"transactions\"\n",
    "\n",
    "raw = (spark.readStream\n",
    "       .format(\"kafka\")\n",
    "       .option(\"kafka.bootstrap.servers\", BOOTSTRAP)\n",
    "       .option(\"subscribe\", TOPIC_IN)\n",
    "       .option(\"startingOffsets\", \"earliest\")\n",
    "       .load())\n",
    "\n",
    "# Kafka value is bytes → string → JSON → columns\n",
    "parsed = (raw.selectExpr(\"CAST(value AS STRING) AS json_str\")\n",
    "            .select(F.from_json(F.col(\"json_str\"), txn_schema).alias(\"r\"))\n",
    "            .select(\"r.*\"))\n",
    "\n",
    "parsed.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0047070",
   "metadata": {},
   "source": [
    "## 5) Streaming inference + alerts\n",
    "- Load the saved model\n",
    "- Compute fraud probability (`fraud_prob`)\n",
    "- Trigger alert when `fraud_prob >= threshold`\n",
    "- Write alerts to:\n",
    "  1) Kafka topic `fraud_alerts`\n",
    "  2) Parquet in `/home/jovyan/work/output_alerts/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63bfd5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started alert streams: to Kafka and to Parquet.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.linalg import Vector, VectorUDT\n",
    "\n",
    "loaded_model = PipelineModel.load(model_path)\n",
    "\n",
    "threshold = 0.90\n",
    "TOPIC_OUT = \"fraud_alerts\"\n",
    "\n",
    "# Define UDF to extract probability for class 1 (fraud)\n",
    "def extract_prob(v):\n",
    "    try:\n",
    "        return float(v[1])\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "extract_prob_udf = F.udf(extract_prob, DoubleType())\n",
    "\n",
    "scored = loaded_model.transform(parsed)\n",
    "scored = scored.withColumn(\"fraud_prob\", extract_prob_udf(F.col(\"probability\")))\n",
    "\n",
    "alerts = (scored\n",
    "          .withColumn(\"is_alert\", (F.col(\"fraud_prob\") >= F.lit(threshold)).cast(\"int\"))\n",
    "          .filter(F.col(\"is_alert\") == 1)\n",
    "          .select(\"event_time\",\"Amount\",\"Class\",\"fraud_prob\",\"is_alert\"))\n",
    "\n",
    "# 1) Write to Kafka (value as JSON)\n",
    "alerts_json = (alerts\n",
    "               .select(F.to_json(F.struct(*alerts.columns)).alias(\"value\")))\n",
    "\n",
    "q_kafka = (alerts_json.writeStream\n",
    "           .format(\"kafka\")\n",
    "           .option(\"kafka.bootstrap.servers\", BOOTSTRAP)\n",
    "           .option(\"topic\", TOPIC_OUT)\n",
    "           .option(\"checkpointLocation\", str(CKPT / \"alerts_to_kafka\"))\n",
    "           .outputMode(\"append\")\n",
    "           .start())\n",
    "\n",
    "# 2) Write to Parquet\n",
    "q_parquet = (alerts.writeStream\n",
    "             .format(\"parquet\")\n",
    "             .option(\"path\", str(OUT))\n",
    "             .option(\"checkpointLocation\", str(CKPT / \"alerts_to_parquet\"))\n",
    "             .outputMode(\"append\")\n",
    "             .start())\n",
    "\n",
    "print(\"Started alert streams: to Kafka and to Parquet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72551a9e",
   "metadata": {},
   "source": [
    "## 6) Windowed KPI analytics (event-time + watermark)\n",
    "We compute per-minute KPIs:\n",
    "- n_txn, n_alert, n_fraud_true, TP, FP\n",
    "- precision, recall\n",
    "Watermark handles late data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0551add9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started KPI console stream.\n"
     ]
    }
   ],
   "source": [
    "# KPI stream (no filtering; use all scored records)\n",
    "all_alerts = (scored\n",
    "              .withColumn(\"is_alert\", (F.col(\"fraud_prob\") >= F.lit(threshold)).cast(\"int\"))\n",
    "              .select(\"event_time\",\"Class\",\"is_alert\"))\n",
    "\n",
    "kpi = (all_alerts\n",
    "       .withWatermark(\"event_time\", \"2 minutes\")\n",
    "       .groupBy(F.window(\"event_time\", \"1 minute\"))\n",
    "       .agg(\n",
    "           F.count(\"*\").alias(\"n_txn\"),\n",
    "           F.sum(\"is_alert\").alias(\"n_alert\"),\n",
    "           F.sum(F.when(F.col(\"Class\")==1, 1).otherwise(0)).alias(\"n_fraud_true\"),\n",
    "           F.sum(F.when((F.col(\"is_alert\")==1) & (F.col(\"Class\")==1), 1).otherwise(0)).alias(\"tp\"),\n",
    "           F.sum(F.when((F.col(\"is_alert\")==1) & (F.col(\"Class\")==0), 1).otherwise(0)).alias(\"fp\"),\n",
    "       )\n",
    "       .withColumn(\"precision\", F.when(F.col(\"n_alert\")>0, F.col(\"tp\")/F.col(\"n_alert\")).otherwise(F.lit(None)))\n",
    "       .withColumn(\"recall\", F.when(F.col(\"n_fraud_true\")>0, F.col(\"tp\")/F.col(\"n_fraud_true\")).otherwise(F.lit(None)))\n",
    "      )\n",
    "\n",
    "q_kpi = (kpi.writeStream\n",
    "         .format(\"console\")\n",
    "         .option(\"truncate\", \"false\")\n",
    "         .option(\"checkpointLocation\", str(CKPT / \"kpi_console\"))\n",
    "         .outputMode(\"update\")\n",
    "         .start())\n",
    "\n",
    "print(\"Started KPI console stream.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099e4bfa",
   "metadata": {},
   "source": [
    "### 6.1 Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c72d5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'message': 'Initializing sources',\n",
       "  'isDataAvailable': False,\n",
       "  'isTriggerActive': True},\n",
       " None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_kpi.status, q_kpi.lastProgress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9880263",
   "metadata": {},
   "source": [
    "### 6.2 Stop streams (important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0843ffbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped all streams.\n"
     ]
    }
   ],
   "source": [
    "for q in [q_kpi, q_kafka, q_parquet]:\n",
    "    try:\n",
    "        q.stop()\n",
    "    except Exception as e:\n",
    "        print(\"stop error:\", e)\n",
    "print(\"Stopped all streams.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94409a9d",
   "metadata": {},
   "source": [
    "## 7) Audit outputs (offline)\n",
    "Read Parquet alerts and compute basic counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc5fc702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------+\n",
      "|n_alert_rows|n_fraud_true_in_alerts|\n",
      "+------------+----------------------+\n",
      "|0           |NULL                  |\n",
      "+------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alerts_saved = spark.read.parquet(str(OUT))\n",
    "alerts_saved.createOrReplaceTempView(\"alerts_saved\")\n",
    "\n",
    "spark.sql('''\n",
    "SELECT\n",
    "  COUNT(*) AS n_alert_rows,\n",
    "  SUM(CASE WHEN Class=1 THEN 1 ELSE 0 END) AS n_fraud_true_in_alerts\n",
    "FROM alerts_saved\n",
    "''').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15eb4ff",
   "metadata": {},
   "source": [
    "## 8) Exercises\n",
    "1) Tune threshold: 0.7 / 0.8 / 0.9 / 0.95. Compare precision/recall.\n",
    "2) Change watermark to 30 seconds. Explain what changes.\n",
    "3) Write KPI to Kafka as another topic (optional).\n",
    "4) Restart the notebook kernels and rerun streams. Explain how checkpointing changes behavior.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
