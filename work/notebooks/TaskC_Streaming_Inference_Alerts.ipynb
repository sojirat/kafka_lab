{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task C: Streaming Inference + Alerts\n",
    "\n",
    "**Objectives:**\n",
    "1. Read transactions from Kafka topic `transactions`\n",
    "2. Parse JSON messages and apply ML model for real-time scoring\n",
    "3. Generate fraud alerts based on threshold\n",
    "4. Write alerts to:\n",
    "   - Kafka topic `fraud_alerts`\n",
    "   - Parquet files (for batch analysis)\n",
    "5. Monitor and verify streaming pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths configured:\n",
      "  Models: /home/jovyan/work/models\n",
      "  Alerts output: /home/jovyan/work/output_alerts\n",
      "  Checkpoints: /home/jovyan/work/checkpoints\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, TimestampType\n",
    "from pyspark.ml import PipelineModel\n",
    "import pyspark\n",
    "import json\n",
    "\n",
    "# Setup paths\n",
    "WORK = Path(\"/home/jovyan/work\")\n",
    "MODELS = WORK / \"models\"\n",
    "OUT = WORK / \"output_alerts\"\n",
    "CKPT = WORK / \"checkpoints\"\n",
    "\n",
    "for p in [MODELS, OUT, CKPT]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Paths configured:\")\n",
    "print(f\"  Models: {MODELS}\")\n",
    "print(f\"  Alerts output: {OUT}\")\n",
    "print(f\"  Checkpoints: {CKPT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Spark with Kafka Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SPARK SESSION INITIALIZED\n",
      "================================================================================\n",
      "Spark version: 3.5.0\n",
      "Kafka package: org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "spark_version = pyspark.__version__\n",
    "kafka_pkg = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{spark_version}\"\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"TaskC-Streaming-Fraud-Detection\")\n",
    "         .master(\"local[*]\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "         .config(\"spark.jars.packages\", kafka_pkg)\n",
    "         .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SPARK SESSION INITIALIZED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Spark version: {spark_version}\")\n",
    "print(f\"Kafka package: {kafka_pkg}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model Selection Summary\n",
    "\n",
    "Load the model and threshold selected in Task A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Selection Summary (from Task A):\n",
      "  Selected Model: Random Forest\n",
      "  PR-AUC: 0.784108\n",
      "  Recommended Threshold: 0.50\n",
      "\n",
      "Model path: /home/jovyan/work/models/fraud_rf_model\n",
      "Alert threshold: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Load model selection summary from Task A\n",
    "audit_path = WORK / \"audit_results\" / \"model_selection_summary.json\"\n",
    "\n",
    "if audit_path.exists():\n",
    "    with open(audit_path, \"r\") as f:\n",
    "        model_summary = json.load(f)\n",
    "    \n",
    "    selected_model_name = model_summary[\"selected_model\"]\n",
    "    pr_auc = model_summary[\"pr_auc\"]\n",
    "    recommended_threshold = model_summary[\"recommended_threshold\"]\n",
    "    \n",
    "    print(\"Model Selection Summary (from Task A):\")\n",
    "    print(f\"  Selected Model: {selected_model_name}\")\n",
    "    print(f\"  PR-AUC: {pr_auc:.6f}\")\n",
    "    print(f\"  Recommended Threshold: {recommended_threshold:.2f}\")\n",
    "    \n",
    "    # Determine model path\n",
    "    if \"Logistic\" in selected_model_name:\n",
    "        model_path = str(MODELS / \"fraud_lr_model\")\n",
    "    else:\n",
    "        model_path = str(MODELS / \"fraud_rf_model\")\n",
    "else:\n",
    "    print(\"WARNING: Model selection summary not found. Using defaults.\")\n",
    "    model_path = str(MODELS / \"fraud_lr_model\")\n",
    "    recommended_threshold = 0.5\n",
    "\n",
    "print(f\"\\nModel path: {model_path}\")\n",
    "print(f\"Alert threshold: {recommended_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Transaction Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction schema defined:\n",
      "  Total fields: 32\n",
      "  Feature fields (V1-V28 + Amount): 29\n",
      "  Label field: Class\n",
      "  Event time field: event_time\n"
     ]
    }
   ],
   "source": [
    "# Define schema for incoming JSON messages\n",
    "schema_fields = [\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"Time\", DoubleType(), True),\n",
    "]\n",
    "\n",
    "for i in range(1, 29):\n",
    "    schema_fields.append(StructField(f\"V{i}\", DoubleType(), True))\n",
    "\n",
    "schema_fields += [\n",
    "    StructField(\"Amount\", DoubleType(), True),\n",
    "    StructField(\"Class\", IntegerType(), True),\n",
    "]\n",
    "\n",
    "txn_schema = StructType(schema_fields)\n",
    "\n",
    "print(\"Transaction schema defined:\")\n",
    "print(f\"  Total fields: {len(schema_fields)}\")\n",
    "print(f\"  Feature fields (V1-V28 + Amount): 29\")\n",
    "print(f\"  Label field: Class\")\n",
    "print(f\"  Event time field: event_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Read Streaming Data from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kafka configuration:\n",
      "  Bootstrap server: kafka:9092\n",
      "  Input topic: transactions\n",
      "  Output topic: fraud_alerts\n",
      "\n",
      "Stream created: True\n",
      "Schema: struct<key:binary,value:binary,topic:string,partition:int,offset:bigint,timestamp:timestamp,timestampType:int>\n"
     ]
    }
   ],
   "source": [
    "BOOTSTRAP = \"kafka:9092\"\n",
    "TOPIC_IN = \"transactions\"\n",
    "TOPIC_OUT = \"fraud_alerts\"\n",
    "\n",
    "print(f\"Kafka configuration:\")\n",
    "print(f\"  Bootstrap server: {BOOTSTRAP}\")\n",
    "print(f\"  Input topic: {TOPIC_IN}\")\n",
    "print(f\"  Output topic: {TOPIC_OUT}\")\n",
    "\n",
    "# Read from Kafka\n",
    "raw = (spark.readStream\n",
    "       .format(\"kafka\")\n",
    "       .option(\"kafka.bootstrap.servers\", BOOTSTRAP)\n",
    "       .option(\"subscribe\", TOPIC_IN)\n",
    "       .option(\"startingOffsets\", \"earliest\")\n",
    "       .load())\n",
    "\n",
    "print(f\"\\nStream created: {raw.isStreaming}\")\n",
    "print(f\"Schema: {raw.schema.simpleString()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parse JSON and Extract Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed stream schema:\n",
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- Time: double (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Kafka value is bytes → convert to string → parse JSON → extract columns\n",
    "parsed = (raw\n",
    "          .selectExpr(\"CAST(value AS STRING) AS json_str\")\n",
    "          .select(F.from_json(F.col(\"json_str\"), txn_schema).alias(\"data\"))\n",
    "          .select(\"data.*\"))\n",
    "\n",
    "print(\"Parsed stream schema:\")\n",
    "parsed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load ML Model and Apply Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from: /home/jovyan/work/models/fraud_rf_model\n",
      "Model stages: ['VectorAssembler', 'RandomForestClassificationModel']\n",
      "\n",
      "Scoring applied to stream\n",
      "Additional columns: prediction, rawPrediction, probability, fraud_prob\n"
     ]
    }
   ],
   "source": [
    "# Load saved model\n",
    "loaded_model = PipelineModel.load(model_path)\n",
    "print(f\"Model loaded from: {model_path}\")\n",
    "print(f\"Model stages: {[type(stage).__name__ for stage in loaded_model.stages]}\")\n",
    "\n",
    "# Define UDF to extract probability for fraud class (class 1)\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def extract_prob(v):\n",
    "    try:\n",
    "        return float(v[1])\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "extract_prob_udf = F.udf(extract_prob, DoubleType())\n",
    "\n",
    "# Apply model to streaming data\n",
    "scored = loaded_model.transform(parsed)\n",
    "scored = scored.withColumn(\"fraud_prob\", extract_prob_udf(F.col(\"probability\")))\n",
    "\n",
    "print(\"\\nScoring applied to stream\")\n",
    "print(\"Additional columns: prediction, rawPrediction, probability, fraud_prob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alert threshold: 0.50\n",
      "Alerts stream created (filtered for fraud_prob >= 0.50)\n"
     ]
    }
   ],
   "source": [
    "# Generate alert flag based on threshold\n",
    "threshold = recommended_threshold\n",
    "\n",
    "alerts = (scored\n",
    "          .withColumn(\"is_alert\", (F.col(\"fraud_prob\") >= F.lit(threshold)).cast(\"int\"))\n",
    "          .filter(F.col(\"is_alert\") == 1)\n",
    "          .select(\n",
    "              \"event_time\",\n",
    "              \"Amount\",\n",
    "              \"Class\",\n",
    "              \"fraud_prob\",\n",
    "              \"is_alert\",\n",
    "              \"prediction\"\n",
    "          ))\n",
    "\n",
    "print(f\"Alert threshold: {threshold:.2f}\")\n",
    "print(f\"Alerts stream created (filtered for fraud_prob >= {threshold:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Write Alerts to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started streaming alerts to Kafka topic: fraud_alerts\n",
      "Query name: None\n",
      "Query ID: 31b47a62-47bc-435d-ab99-0964d62bf830\n"
     ]
    }
   ],
   "source": [
    "# Convert alerts to JSON for Kafka\n",
    "alerts_json = alerts.select(\n",
    "    F.to_json(F.struct(\n",
    "        F.col(\"event_time\").cast(\"string\").alias(\"event_time\"),\n",
    "        \"Amount\",\n",
    "        \"Class\",\n",
    "        \"fraud_prob\",\n",
    "        \"is_alert\",\n",
    "        \"prediction\"\n",
    "    )).alias(\"value\")\n",
    ")\n",
    "\n",
    "# Start writing to Kafka\n",
    "query_kafka = (alerts_json.writeStream\n",
    "               .format(\"kafka\")\n",
    "               .option(\"kafka.bootstrap.servers\", BOOTSTRAP)\n",
    "               .option(\"topic\", TOPIC_OUT)\n",
    "               .option(\"checkpointLocation\", str(CKPT / \"alerts_to_kafka\"))\n",
    "               .outputMode(\"append\")\n",
    "               .start())\n",
    "\n",
    "print(f\"Started streaming alerts to Kafka topic: {TOPIC_OUT}\")\n",
    "print(f\"Query name: {query_kafka.name}\")\n",
    "print(f\"Query ID: {query_kafka.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Write Alerts to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started streaming alerts to Parquet: /home/jovyan/work/output_alerts\n",
      "Query name: None\n",
      "Query ID: fa7a7ed4-21f6-4158-a7ee-b993c7ceea5f\n"
     ]
    }
   ],
   "source": [
    "# Write to Parquet for batch analysis\n",
    "query_parquet = (alerts.writeStream\n",
    "                 .format(\"parquet\")\n",
    "                 .option(\"path\", str(OUT))\n",
    "                 .option(\"checkpointLocation\", str(CKPT / \"alerts_to_parquet\"))\n",
    "                 .outputMode(\"append\")\n",
    "                 .start())\n",
    "\n",
    "print(f\"Started streaming alerts to Parquet: {OUT}\")\n",
    "print(f\"Query name: {query_parquet.name}\")\n",
    "print(f\"Query ID: {query_parquet.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Write Console Output (for monitoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started console output for monitoring\n"
     ]
    }
   ],
   "source": [
    "# Optional: Write to console for real-time monitoring\n",
    "query_console = (alerts.writeStream\n",
    "                 .format(\"console\")\n",
    "                 .option(\"truncate\", \"false\")\n",
    "                 .option(\"numRows\", \"10\")\n",
    "                 .outputMode(\"append\")\n",
    "                 .start())\n",
    "\n",
    "print(\"Started console output for monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Monitor Streaming Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STREAMING QUERIES STATUS\n",
      "================================================================================\n",
      "\n",
      "Kafka Query:\n",
      "  ID: 31b47a62-47bc-435d-ab99-0964d62bf830\n",
      "  Status: Getting offsets from KafkaV2[Subscribe[transactions]]\n",
      "  Is Active: True\n",
      "  Data Available: False\n",
      "\n",
      "Parquet Query:\n",
      "  ID: fa7a7ed4-21f6-4158-a7ee-b993c7ceea5f\n",
      "  Status: Getting offsets from KafkaV2[Subscribe[transactions]]\n",
      "  Is Active: True\n",
      "  Data Available: False\n",
      "\n",
      "Console Query:\n",
      "  ID: 5fe50ba0-390d-4f31-aa0d-681a6dc754a5\n",
      "  Status: Getting offsets from KafkaV2[Subscribe[transactions]]\n",
      "  Is Active: True\n",
      "  Data Available: False\n",
      "\n",
      "================================================================================\n",
      "Queries are running. Let them process for 30 seconds...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Monitor query status\n",
    "print(\"=\"*80)\n",
    "print(\"STREAMING QUERIES STATUS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "queries = [query_kafka, query_parquet, query_console]\n",
    "query_names = [\"Kafka\", \"Parquet\", \"Console\"]\n",
    "\n",
    "for name, q in zip(query_names, queries):\n",
    "    status = q.status\n",
    "    print(f\"\\n{name} Query:\")\n",
    "    print(f\"  ID: {q.id}\")\n",
    "    print(f\"  Status: {status['message']}\")\n",
    "    print(f\"  Is Active: {q.isActive}\")\n",
    "    print(f\"  Data Available: {status.get('isDataAvailable', 'N/A')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Queries are running. Let them process for 30 seconds...\")\n",
    "print(\"=\"*80)\n",
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUERY PROGRESS\n",
      "================================================================================\n",
      "\n",
      "Kafka Query: No progress data yet\n",
      "\n",
      "Parquet Query: No progress data yet\n",
      "\n",
      "Console Query: No progress data yet\n"
     ]
    }
   ],
   "source": [
    "# Check progress\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUERY PROGRESS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, q in zip(query_names, queries):\n",
    "    progress = q.lastProgress\n",
    "    if progress:\n",
    "        print(f\"\\n{name} Query Progress:\")\n",
    "        print(f\"  Batch: {progress.get('batchId', 'N/A')}\")\n",
    "        print(f\"  Input rows: {progress.get('numInputRows', 'N/A')}\")\n",
    "        print(f\"  Processing time: {progress.get('durationMs', {}).get('triggerExecution', 'N/A')} ms\")\n",
    "    else:\n",
    "        print(f\"\\n{name} Query: No progress data yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Consume Alerts from Kafka (Verification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAMPLE ALERTS FROM KAFKA\n",
      "================================================================================\n",
      "+----------+------+-----+----------+--------+----------+\n",
      "|event_time|Amount|Class|fraud_prob|is_alert|prediction|\n",
      "+----------+------+-----+----------+--------+----------+\n",
      "+----------+------+-----+----------+--------+----------+\n",
      "\n",
      "\n",
      "Total alerts in Kafka topic: 678\n"
     ]
    }
   ],
   "source": [
    "# Read alerts from Kafka topic to verify\n",
    "alerts_from_kafka = (spark.read\n",
    "                     .format(\"kafka\")\n",
    "                     .option(\"kafka.bootstrap.servers\", BOOTSTRAP)\n",
    "                     .option(\"subscribe\", TOPIC_OUT)\n",
    "                     .option(\"startingOffsets\", \"earliest\")\n",
    "                     .load())\n",
    "\n",
    "# Parse JSON value\n",
    "alerts_parsed = (alerts_from_kafka\n",
    "                 .selectExpr(\"CAST(value AS STRING) AS json_str\")\n",
    "                 .select(F.from_json(F.col(\"json_str\"), \n",
    "                                     StructType([\n",
    "                                         StructField(\"event_time\", TimestampType()),\n",
    "                                         StructField(\"Amount\", DoubleType()),\n",
    "                                         StructField(\"Class\", IntegerType()),\n",
    "                                         StructField(\"fraud_prob\", DoubleType()),\n",
    "                                         StructField(\"is_alert\", IntegerType()),\n",
    "                                         StructField(\"prediction\", DoubleType())\n",
    "                                     ])).alias(\"alert\"))\n",
    "                 .select(\"alert.*\"))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE ALERTS FROM KAFKA\")\n",
    "print(\"=\"*80)\n",
    "alerts_parsed.show(10, truncate=False)\n",
    "\n",
    "alert_count = alerts_parsed.count()\n",
    "print(f\"\\nTotal alerts in Kafka topic: {alert_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Verify Parquet Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PARQUET OUTPUT VERIFICATION\n",
      "================================================================================\n",
      "Output directory: /home/jovyan/work/output_alerts\n",
      "Number of parquet files: 3\n",
      "\n",
      "Sample alerts from Parquet:\n",
      "+----------+------+-----+----------+--------+\n",
      "|event_time|Amount|Class|fraud_prob|is_alert|\n",
      "+----------+------+-----+----------+--------+\n",
      "+----------+------+-----+----------+--------+\n",
      "\n",
      "\n",
      "Total alerts in Parquet: 0\n",
      "\n",
      "Alert Statistics:\n",
      "+-------+----------+------+\n",
      "|summary|fraud_prob|Amount|\n",
      "+-------+----------+------+\n",
      "|  count|         0|     0|\n",
      "|   mean|      NULL|  NULL|\n",
      "| stddev|      NULL|  NULL|\n",
      "|    min|      NULL|  NULL|\n",
      "|    max|      NULL|  NULL|\n",
      "+-------+----------+------+\n",
      "\n",
      "\n",
      "Alert Quality:\n",
      "+-----+-----+\n",
      "|Class|count|\n",
      "+-----+-----+\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if parquet files exist\n",
    "parquet_files = list(OUT.glob(\"*.parquet\"))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARQUET OUTPUT VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Output directory: {OUT}\")\n",
    "print(f\"Number of parquet files: {len(parquet_files)}\")\n",
    "\n",
    "if parquet_files:\n",
    "    # Read alerts from parquet\n",
    "    alerts_from_parquet = spark.read.parquet(str(OUT))\n",
    "    \n",
    "    print(\"\\nSample alerts from Parquet:\")\n",
    "    alerts_from_parquet.show(10, truncate=False)\n",
    "    \n",
    "    parquet_count = alerts_from_parquet.count()\n",
    "    print(f\"\\nTotal alerts in Parquet: {parquet_count}\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\nAlert Statistics:\")\n",
    "    alerts_from_parquet.describe([\"fraud_prob\", \"Amount\"]).show()\n",
    "    \n",
    "    # True vs False positives\n",
    "    print(\"\\nAlert Quality:\")\n",
    "    alerts_from_parquet.groupBy(\"Class\").count().show()\n",
    "else:\n",
    "    print(\"\\nNo parquet files found yet. Query may still be processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Alert Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ALERT ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "No alerts generated yet\n"
     ]
    }
   ],
   "source": [
    "if parquet_files:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ALERT ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # True Positives vs False Positives\n",
    "    tp = alerts_from_parquet.filter(F.col(\"Class\") == 1).count()\n",
    "    fp = alerts_from_parquet.filter(F.col(\"Class\") == 0).count()\n",
    "    total_alerts = tp + fp\n",
    "    \n",
    "    if total_alerts > 0:\n",
    "        precision = tp / total_alerts\n",
    "        print(f\"\\nAlert Breakdown:\")\n",
    "        print(f\"  Total alerts: {total_alerts}\")\n",
    "        print(f\"  True Positives (TP): {tp}\")\n",
    "        print(f\"  False Positives (FP): {fp}\")\n",
    "        print(f\"  Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "        \n",
    "        print(f\"\\nInterpretation:\")\n",
    "        if precision >= 0.5:\n",
    "            print(f\"  ✓ Good precision: {precision*100:.1f}% of alerts are true fraud\")\n",
    "        else:\n",
    "            print(f\"  ⚠ Low precision: {precision*100:.1f}% of alerts are true fraud\")\n",
    "            print(f\"  Consider increasing threshold or improving model\")\n",
    "    else:\n",
    "        print(\"\\nNo alerts generated yet\")\n",
    "else:\n",
    "    print(\"\\nWaiting for alert data...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Stop Streaming Queries\n",
    "\n",
    "**IMPORTANT:** Always stop queries before shutting down or restarting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping all streaming queries...\n",
      "  ✓ Kafka query stopped\n",
      "  ✓ Parquet query stopped\n",
      "  ✓ Console query stopped\n",
      "\n",
      "All queries stopped.\n"
     ]
    }
   ],
   "source": [
    "print(\"Stopping all streaming queries...\")\n",
    "\n",
    "for name, q in zip(query_names, queries):\n",
    "    try:\n",
    "        q.stop()\n",
    "        print(f\"  ✓ {name} query stopped\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error stopping {name} query: {e}\")\n",
    "\n",
    "print(\"\\nAll queries stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Task C Deliverables\n",
    "\n",
    "### Deliverable C.1: Screenshot of running stream\n",
    "Take a screenshot of the console output showing alerts being processed.\n",
    "\n",
    "### Deliverable C.2: Sample alert messages\n",
    "See the \"Sample alerts from Kafka\" section above for at least 5 alert examples.\n",
    "\n",
    "Key alert fields:\n",
    "- `event_time`: When the transaction occurred\n",
    "- `Amount`: Transaction amount\n",
    "- `Class`: True label (0=normal, 1=fraud)\n",
    "- `fraud_prob`: Model's fraud probability\n",
    "- `is_alert`: Alert flag (1 if fraud_prob >= threshold)\n",
    "- `prediction`: Model's binary prediction\n",
    "\n",
    "### Pipeline Summary\n",
    "- Input: Kafka topic `transactions`\n",
    "- Processing: ML model scoring in real-time\n",
    "- Output 1: Kafka topic `fraud_alerts` (for downstream consumers)\n",
    "- Output 2: Parquet files (for batch analysis)\n",
    "- Checkpointing: Enabled for fault tolerance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
